{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Repository Scraper\n",
    "Scrapes all the topics from github, stores their topic_name, topic_url and description in csv file\n",
    "For each topic mentioned above, scraper scrapes top 30 repositories according to stars and stores their username, repo_name, repo_url, stars, description(if provided)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pandas\n",
    "!pip install bs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def convertStarsToInt(string):\n",
    "    string = string.strip()\n",
    "    if (string[-1] == 'k'): # if last char is k (thousand)\n",
    "        return int(float(string[:-1]) * 1000)\n",
    "    return int(string)\n",
    "\n",
    "def getRepoInfo(h3_tag, star_tag, description):\n",
    "    a_tags = h3_tag.find_all(\"a\")\n",
    "    repo_url = \"https://github.com\" + a_tags[1][\"href\"]\n",
    "    username = a_tags[0].text.strip()\n",
    "    repo_name = a_tags[1].text.strip()\n",
    "    stars = convertStarsToInt(star_tag.text.strip())\n",
    "    \n",
    "    desc = description.find( # get the description of the repository\n",
    "        \"div\",\n",
    "        {\"class\": \"color-bg-primary\"})\n",
    "    desc = desc.find(\n",
    "        \"div\",\n",
    "        {\"class\": \"px-3 pt-3\"})\n",
    "    if (desc == None): # if description is not given by repo owner\n",
    "        return repo_url, username, repo_name, stars, \"\"\n",
    "    desc = desc.find(\n",
    "        \"div\"\n",
    "        )\n",
    "    \n",
    "    return repo_url, username, repo_name, stars, desc.text.strip()\n",
    "\n",
    "def getTopicRepos(topic_url):\n",
    "    # Download the page\n",
    "    flag = True\n",
    "    \n",
    "    while (flag):\n",
    "        response = requests.get(topic_url) # response is an object here\n",
    "        if (response.status_code != 200): # Check whether response is OK\n",
    "            #raise Exception(\"Failed to load page \" + topics_url)\n",
    "            flag = True\n",
    "        else:\n",
    "            flag = False\n",
    "        \n",
    "    # Parse using BeautifulSoup\n",
    "    topic_doc = BeautifulSoup(response.text, \"html.parser\")\n",
    "    h3_tags = topic_doc.find_all(\n",
    "                \"h3\",\n",
    "                {\"class\": \"f3 color-text-secondary text-normal lh-condensed\"}\n",
    "                )\n",
    "    stars = topic_doc.find_all(\n",
    "            \"a\",\n",
    "            \"social-count float-none\"\n",
    "            )\n",
    "    \n",
    "    \n",
    "    description = topic_doc.find_all(\n",
    "    \"article\",\n",
    "    {\"class\": \"border rounded color-shadow-small color-bg-secondary my-4\"}\n",
    "    )\n",
    "  \n",
    "    \n",
    "    topic_repos_dict = {\n",
    "        \"username\": [],\n",
    "        \"repo_name\": [],\n",
    "        \"repo_url\": [],\n",
    "        \"stars\": [],\n",
    "        \"description\": []\n",
    "    }\n",
    "\n",
    "    for i in range(len(h3_tags)):\n",
    "        repo_info = getRepoInfo(h3_tags[i], stars[i], description[i])\n",
    "        topic_repos_dict[\"username\"].append(repo_info[1])\n",
    "        topic_repos_dict[\"repo_name\"].append(repo_info[2])\n",
    "        topic_repos_dict[\"repo_url\"].append(repo_info[0])\n",
    "        topic_repos_dict[\"stars\"].append(repo_info[3])\n",
    "        topic_repos_dict[\"description\"].append(repo_info[4])\n",
    "    \n",
    "    return pd.DataFrame(topic_repos_dict)\n",
    "\n",
    "def scrapeTopicsRepos(): # scrape for all the topics, all the repos\n",
    "    topicsUrl = \"https://github.com/topics\"\n",
    "    pageNumber = 1\n",
    "    \n",
    "    \n",
    "    topic_titles = []\n",
    "    topic_descriptions = []\n",
    "    topic_urls = []\n",
    "    print(\"Fetching Topics...\")\n",
    "    while (pageNumber <= 7): # On github we have a \"Load More\" button, when we click there the url changes to /?page=2, /?page=3, we use this url to access furthur topics\n",
    "        flag = True\n",
    "        while (flag):\n",
    "            response = requests.get(topicsUrl + \"?page=\" + str(pageNumber)) # response is an object here\n",
    "            if (response.status_code != 200): # Check whether response is OK\n",
    "                #raise Exception(\"Failed to load page \" + topics_url)\n",
    "                flag = True\n",
    "            else:\n",
    "                flag = False\n",
    "    \n",
    "        parsedDoc = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        topic_title_tags = parsedDoc.find_all(\n",
    "            \"p\", \n",
    "            {\"class\": \"f3 lh-condensed mb-0 mt-1 Link--primary\"}\n",
    "        )\n",
    "\n",
    "        topic_description_tags = parsedDoc.find_all(\n",
    "            \"p\",\n",
    "            {\"class\": \"f5 color-text-secondary mb-0 mt-1\"}\n",
    "        )\n",
    "\n",
    "        topic_divs = parsedDoc.find_all(\n",
    "            \"div\",\n",
    "            {\"class\": \"py-4 border-bottom\"}\n",
    "        )\n",
    "\n",
    "        topic_link_tags = [] # inside the divs, find the link tags\n",
    "        for i in range(len(topic_divs)):\n",
    "          topic_link_tags += topic_divs[i].find_all(\"a\", recursive = False)\n",
    "\n",
    "\n",
    "        for tag in topic_title_tags:\n",
    "          topic_titles.append(tag.text) # tag.text gives innerText of a tag\n",
    "\n",
    "\n",
    "\n",
    "        for tag in topic_description_tags:\n",
    "          topic_descriptions.append(tag.text.strip()) # .strip() removes all empty space in beginning and end\n",
    "\n",
    "\n",
    "\n",
    "        for tag in topic_link_tags:\n",
    "          topic_urls.append(\"https://github.com\" + tag[\"href\"])\n",
    "        pageNumber += 1\n",
    "        \n",
    "    \n",
    "    topics_dict = {\n",
    "        \"title\": topic_titles, # first column\n",
    "        \"description\": topic_descriptions, # second column\n",
    "        \"url\": topic_urls # third column\n",
    "    }\n",
    "    \n",
    "    # topics_df is HERE\n",
    "    topics_df = pd.DataFrame(topics_dict)\n",
    "    \n",
    "    os.mkdir(\"Repository-Scraper\")\n",
    "    os.mkdir(\"Repository-Scraper/topics\")\n",
    "    topics_df.to_csv(\"Repository-Scraper/topics.csv\", index = None)\n",
    "    \n",
    "    for i in range(len(topic_urls)):\n",
    "        print(\"Fetching top repositories for the topic \" + topic_titles[i])\n",
    "        getTopicRepos(topic_urls[i]).to_csv(\"Repository-Scraper/topics/\" + topic_titles[i] + \".csv\", index = None)\n",
    "    return\n",
    "\n",
    "scrapeTopicsRepos()   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Future work to be done:\n",
    "Also get all other tags from the repositories"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
